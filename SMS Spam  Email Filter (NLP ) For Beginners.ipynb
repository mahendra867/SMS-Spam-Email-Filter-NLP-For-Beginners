{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce010fb",
   "metadata": {},
   "source": [
    "### NLP (Natural Language Processing) with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78d2f4",
   "metadata": {},
   "source": [
    "- Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "- In this article, we will discuss a higher-level overview of the basics of Natural Language Processing, which basically consists of combining machine learning techniques with text, and using math and statistics to get that text in a format that the machine learning algorithms can understand!\n",
    "\n",
    "### üìù Agenda\n",
    "1. Representing text as numerical data\n",
    "2. Reading a text-based dataset into pandas\n",
    "3. Vectorizing our dataset\n",
    "4. Building and evaluating a model\n",
    "5. Comparing models\n",
    "6. Examining a model for further insight\n",
    "7. Practicing this workflow on another dataset\n",
    "8. Tuning the vectorizer (discussion)\n",
    "\n",
    "### üìå Notebook Goals\n",
    "In this notebook we will discuss a higher level overview of the basics of Natural Language Processing, which basically consists of combining machine learning techniques with text, and using math and statistics to get that text in a format that the machine learning algorithms can understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22d48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\") # set_style() is a function in Seaborn used to set the visual style of plots. In this command, you are setting the visual style to \"whitegrid.\" Seaborn provides several built-in styles that change the appearance of your plots, including \"whitegrid.\" This style typically results in plots with a white background and subtle gridlines, which can be useful for visualizing data.\n",
    "plt.style.use(\"fivethirtyeight\") # style is a submodule in Matplotlib that allows you to set various plotting styles. In this command, you are setting the Matplotlib style to \"fivethirtyeight.\" This style emulates the visual style used by the popular data journalism website FiveThirtyEight. It typically results in plots with distinctive colors, fonts, and formatting that are reminiscent of FiveThirtyEight's visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c22df",
   "metadata": {},
   "source": [
    "### üîÅ Representing text as numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea84f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text for model training (SMS messages)\n",
    "simple_train=['call you tonight','Call me a cab','Please call me.. PLEASE!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04ee73",
   "metadata": {},
   "source": [
    "### üìå From the  [scikit-learn documentation:](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "- Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length.**\n",
    "\n",
    "We will use CountVectorizer to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355c9868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cab', 'call', 'me', 'please', 'tonight', 'you'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect=CountVectorizer()\n",
    "\n",
    "# learn the 'vocabulary' of the training data (occurs in-place)\n",
    "\n",
    "vect.fit(simple_train)\n",
    "\n",
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f125387",
   "metadata": {},
   "source": [
    "The code you provided is related to text preprocessing and feature extraction, specifically using the `CountVectorizer` from the Scikit-Learn library in Python. Let's break down the code and explain each part:\n",
    "\n",
    "1. **Simple Training Data**:\n",
    "   ```python\n",
    "   simple_train = ['call you tonight', 'Call me a cab', 'Please call me.. PLEASE!']\n",
    "   ```\n",
    "   These are three simple text messages. This is the training data you'll use to demonstrate how the `CountVectorizer` works. Each element in the `simple_train` list represents a text message.\n",
    "\n",
    "2. **Import and Instantiate CountVectorizer**:\n",
    "   ```python\n",
    "   from sklearn.feature_extraction.text import CountVectorizer\n",
    "   vect = CountVectorizer()\n",
    "   ```\n",
    "   - `from sklearn.feature_extraction.text import CountVectorizer`: This line imports the `CountVectorizer` class from Scikit-Learn, which is a tool for converting text data into numerical features that can be used in machine learning models.\n",
    "   - `vect = CountVectorizer()`: This line creates an instance of the `CountVectorizer` class and stores it in the variable `vect`. The `CountVectorizer` will be used to process and convert the text data.\n",
    "\n",
    "3. **Learning the Vocabulary**:\n",
    "   ```python\n",
    "   vect.fit(simple_train)\n",
    "   ```\n",
    "   - `vect.fit(simple_train)`: This line fits (or trains) the `CountVectorizer` on the provided `simple_train` data. In this step, the `CountVectorizer` analyzes the text data and builds a vocabulary. It identifies unique words (or tokens) in the text and assigns an index to each word in the vocabulary.\n",
    "\n",
    "4. **Examine the Fitted Vocabulary**:\n",
    "   ```python\n",
    "   vect.get_feature_names_out()\n",
    "   ```\n",
    "   - `vect.get_feature_names_out()`: This line retrieves the feature names (words) that were identified by the `CountVectorizer` during the training process. These feature names are the unique words found in the training data, and they will be used as columns in the feature matrix when you transform new text data.\n",
    "\n",
    "In your specific example, the fitted vocabulary contains the following words: 'cab', 'call', 'me', 'please', 'tonight', 'you'. These words were extracted from the training text messages.\n",
    "\n",
    "Overall, the `CountVectorizer` is a common preprocessing step in natural language processing and text analysis. It converts text data into a numerical format that machine learning models can understand by representing each document (text message, in this case) as a vector of word counts. In the fitted vocabulary, each word corresponds to a unique feature, and the presence or absence of each word in a document is used to create a numerical representation of the document. This is a basic way to convert text data into a format suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0e30e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef60d282",
   "metadata": {},
   "source": [
    "### Concept of the CountVectorizer\n",
    "\n",
    "- Certainly! The `CountVectorizer` is a tool used in natural language processing (NLP) and text analysis to turn text data into numbers that a computer can understand. Here's a simple explanation of how it works:\n",
    "\n",
    "Imagine you have a bunch of text messages, like these:\n",
    "\n",
    "1. \"Call me a cab.\"\n",
    "2. \"Please call me.\"\n",
    "3. \"Call you tonight.\"\n",
    "\n",
    "The `CountVectorizer` helps you convert these messages into something a computer can use. Here's what it does step by step:\n",
    "\n",
    "1. **Tokenization**: It breaks down each message into individual words. For example, \"Call me a cab\" becomes [\"Call\", \"me\", \"a\", \"cab\"].\n",
    "\n",
    "2. **Counting**: It counts how many times each word appears in each message. For example, in the first message, \"Call\" appears once, \"me\" appears once, \"a\" appears once, and \"cab\" appears once.\n",
    "\n",
    "3. **Creating a Table**: It builds a table where each row represents a message, and each column represents a word from the messages. It then fills in the table with the counts from step 2. \n",
    "\n",
    "   For example:\n",
    "\n",
    "\n",
    "   |  Message          | Call | me | a | cab | Please | you | tonight |\n",
    "   |-------------------|------|----|---|-----|--------|-----|---------|\n",
    "   | \"Call me a cab.\" |  1   | 1  | 1 |  1  |   0    |  0  |   0     |\n",
    "   | \"Please call me.\" |  1   | 1  | 0 |  0  |   1    |  0  |   0     |\n",
    "   | \"Call you tonight.\"|  1   | 0  | 0 |  0  |   0    |  1  |   1     |\n",
    " \n",
    " \n",
    "\n",
    "4. **Numeric Representation**: This table becomes a numeric representation of your text data. You can now use these numbers as input for machine learning algorithms. Each row represents a message, and each number in the row represents how many times a specific word appears in that message.\n",
    "\n",
    "So, in simple terms, the `CountVectorizer` helps you turn text into a table of numbers, where each number tells you how many times a word appears in a message. This is useful because many machine learning algorithms work with numbers, not text, so this transformation allows you to use text data for tasks like classification or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e9fe59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm=vect.transform(simple_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffedd2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fedc3c",
   "metadata": {},
   "source": [
    "**Converting a sparse matrix to a dense matrix** means transforming a data structure where most of the elements are zeros (sparse) into a data structure where all elements are explicitly stored (dense). This operation can be important in various data processing and numerical computation tasks. Here's what it means in more detail:\n",
    "\n",
    "**Sparse Matrix:**\n",
    "\n",
    "In a sparse matrix, most of the elements have a value of zero.\n",
    "Storing all these zeros can be inefficient in terms of memory usage, especially for large matrices.\n",
    "Sparse matrices are often used when dealing with data that has a lot of missing or zero values, such as in text data, network graphs, or certain scientific datasets.\n",
    "\n",
    "**Dense Matrix:**\n",
    "\n",
    "In a dense matrix, all elements are explicitly stored, regardless of whether they are zero or non-zero.\n",
    "Dense matrices take up more memory compared to sparse matrices, but they are straightforward to work with and are the standard representation in most numerical libraries and algorithms.\n",
    "Dense matrices are commonly used in linear algebra and numerical computations.\n",
    "\n",
    "**Converting a sparse matrix to a dense matrix involves filling in all the missing (zero) values and representing the matrix as a regular two-dimensional array or data structure where every element is stored explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba32dea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3ca5f",
   "metadata": {},
   "source": [
    "üìå From the [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "In this scheme, features and samples are defined as follows:\n",
    "\n",
    "- Each individual token occurrence frequency (normalized or not) is treated as a **feature.**\n",
    "- The vector of all the token frequencies for a given document is considered a **multivariate sample.**\n",
    "\n",
    "- A **corpus of documents** can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3113510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "# check the type of the document-term matrix\n",
    "print(type(simple_train_dtm))\n",
    "\n",
    "# examine the sparse matrix contents\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5947b",
   "metadata": {},
   "source": [
    "- <class 'scipy.sparse.csr.csr_matrix'>: This indicates that simple_train_dtm is an instance of the csr_matrix class from the SciPy library. The term \"csr\" stands for \"Compressed Sparse Row,\" which is a type of sparse matrix representation. Sparse matrices are used to efficiently store and manipulate matrices where most of the elements are zero.\n",
    "\n",
    "Each line represents a non-zero element in the matrix:\n",
    "\n",
    "- The first number in each line is the row index.\n",
    "- The second number is the column index.\n",
    "- The third number is the value at that position in the matrix.\n",
    "\n",
    "For example, the first line (0, 1) 1 means that in row 0 and column 1 of the matrix, the value is 1. Similarly, the last line (2, 3) 2 means that in row 2 and column 3, the value is 2.\n",
    "\n",
    "- The presence of these non-zero values is what makes this a \"sparse\" matrix. In a dense matrix, you would see all the rows and columns, including zeros, explicitly listed. However, in a sparse matrix, only the non-zero values are stored to save memory and improve efficiency when dealing with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f433b6",
   "metadata": {},
   "source": [
    "### üìå From the [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).\n",
    "\n",
    "For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "In order to be able to store such a matrix in memory but also to speed up operations, implementations will typically use a sparse representation such as the implementations available in the scipy.sparse package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d5a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text for model testing\n",
    "simple_test = [\"please don't call me\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abd4af",
   "metadata": {},
   "source": [
    "In order to make a prediction, the new observation must have the same features as the training observations, both in number and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66221a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test_dtm=vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1295be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(),columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07aeed0",
   "metadata": {},
   "source": [
    "### üìã Summary:\n",
    "- vect.fit(train) **learns the vocabulary** of the training data\n",
    "- vect.transform(train) uses the **fitted vocabulary** to build a document-term matrix from the training data\n",
    "- vect.transform(test) uses the **fitted vocabulary** to build a document-term matrix from the testing data (and ignores tokens it hasn't seen before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b0275",
   "metadata": {},
   "source": [
    "### Observation \n",
    "While testing the new text document which it contain the Dont unique but the model while examine the testing data it dont given any token number to the dont unique word or it ignore that unique word\n",
    "\n",
    "\n",
    "### Reason\n",
    "\n",
    "The vect.transform(test) method, when applied to testing data, ignores tokens (words) it hasn't seen before because it follows the same vocabulary that was learned from the training data. Here's why it works this way:\n",
    "\n",
    "- **Consistency with the Training Data:** In natural language processing and text analysis, it's essential to maintain consistency between the training and testing data. When you train a model or a text processing tool (like CountVectorizer) on a dataset, it learns patterns, features, and vocabulary from that specific dataset. It doesn't have prior knowledge of words or tokens outside of what it has seen in the training data.\n",
    "\n",
    "- **Vocabulary Building:** During the training phase (when you use vect.fit()), the CountVectorizer scans the training data and builds a vocabulary of all unique tokens (words) it encounters. Each token is assigned a unique index in the vocabulary, and the vectorizer keeps track of how many times each token appears in the training documents.\n",
    "\n",
    "- **Testing Data:** When you apply the same vectorizer to the testing data (using vect.transform(test)), it uses the vocabulary it learned during training to map the tokens in the testing documents to their corresponding indices in the vocabulary. If it encounters a token in the testing data that it has never seen during training, it simply ignores it.\n",
    "\n",
    "Ignoring unknown tokens is a common practice because the vectorizer doesn't know how to represent these unknown words. It's better to ignore them than to make assumptions about their meaning or representation\n",
    "\n",
    "- **Maintaining Data Consistency**: By ignoring unseen tokens in the testing data, you ensure that the feature representations (document-term matrix) for both training and testing data have the same dimensions and vocabulary. This consistency is crucial when you train machine learning models because those models expect input data to have consistent features.\n",
    "\n",
    "In summary, the CountVectorizer ignores tokens it hasn't seen before during testing to maintain consistency and avoid making assumptions about the meaning or representation of unknown words. This approach ensures that the testing data is processed using the same vocabulary as the training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99fe22",
   "metadata": {},
   "source": [
    "### üíæ Reading a text-based dataset into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ae8cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read file into pandas using a relative path\n",
    "sms=pd.read_csv(\"C:\\\\Users\\\\mahen\\\\Downloads\\\\spam.csv\",encoding='latin-1')\n",
    "sms.dropna(how='any',inplace=True,axis=1) # .dropna(how='any', inplace=True, axis=1): This line of code is calling the dropna method on the DataFrame sms with the following parameters: how='any': This parameter specifies that you want to drop columns if they contain any NaN (missing) values. In other words, if any column in the DataFrame has at least one missing value, it will be removed. inplace=True: This parameter indicates that you want to modify the DataFrame sms in place, which means the changes will be applied directly to the original DataFrame, and it won't create a new DataFrame. axis=1: This parameter specifies that you want to drop columns (as opposed to rows). When axis=1, it operates on columns.\n",
    "sms.columns=['label','message']\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402ee39",
   "metadata": {},
   "source": [
    "### üîç Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07daa877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec3250",
   "metadata": {},
   "source": [
    "### Explaination \n",
    "- \"4825\" under \"freq\" for the \"label\" column means that the label \"ham\" appears 4825 times in the \"label\" column.\n",
    "\n",
    "- \"30\" under \"freq\" for the \"message\" column means that the message \"Sorry, I'll call later\" appears 30 times in the \"message\" column.\n",
    "\n",
    "- TOP -This shows the most frequently occurring value in a column.\n",
    "\n",
    "- \"ham\" under \"top\" for the \"label\" column means that \"ham\" is the most frequently occurring label in the \"label\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23bc1003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5298582",
   "metadata": {},
   "source": [
    "We have 4825 ham message and 747 spam message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fafdc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['label_num']=sms.label.map({'ham':0,'spam':1})\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ac73d",
   "metadata": {},
   "source": [
    "In the code you provided, a new column called 'label_num' is being added to the DataFrame 'sms' based on the values in the 'label' column. The values 'ham' and 'spam' in the 'label' column are being mapped to 0 and 1, respectively. This kind of encoding is typically done for binary classification tasks, and here's why 'ham' is mapped to 0 and 'spam' is mapped to 1:\n",
    "\n",
    "Binary Classification: In many machine learning tasks, especially in binary classification, you have two classes or categories that you want to predict or analyze. In your case, it seems to be a binary classification problem where you're distinguishing between two types of text messages: 'ham' (non-spam) and 'spam' (spam).\n",
    "\n",
    "Numeric Labels: Many machine learning algorithms and libraries require that the target variable (the variable you're trying to predict) be numeric. In this case, 'ham' and 'spam' are text labels, and to use them as target values for classification, you need to convert them into numeric format.\n",
    "\n",
    "Mapping 'ham' to 0 and 'spam' to 1: The choice of mapping 'ham' to 0 and 'spam' to 1 is somewhat arbitrary but often chosen for convenience:\n",
    "\n",
    "'ham' is mapped to 0 because it represents the negative or non-spam class, and conventionally, 0 is used to represent the negative or \"no\" class in binary classification.\n",
    "\n",
    "'spam' is mapped to 1 because it represents the positive or spam class, and 1 is used to represent the positive or \"yes\" class in binary classification.\n",
    "\n",
    "By using this mapping, you create a binary target variable that is easy to work with in binary classification algorithms. It simplifies the problem to distinguishing between \"not spam\" (0) and \"spam\" (1).\n",
    "\n",
    "Model Training: Once you have this numeric representation of the target variable, you can use it to train machine learning models. These models can then predict whether a given message is 'ham' (0) or 'spam' (1) based on various features extracted from the text.\n",
    "\n",
    "In summary, converting 'ham' to 0 and 'spam' to 1 is a common practice in binary classification tasks to transform text labels into a format that can be used with machine learning algorithms, making it easier to build and train models for spam classification or similar tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9cfe7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2506ba54",
   "metadata": {},
   "source": [
    "**As we continue our analysis we want to start thinking about the features we are going to be using. This goes along with the general idea of feature engineering. The better your domain knowledge on the data, the better your ability to engineer more features from it. Feature engineering is a very large part of spam detection in general.**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e268a757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0   \n",
       "1   ham                      Ok lar... Joking wif u oni...          0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1   \n",
       "3   ham  U dun say so early hor... U c already then say...          0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0   \n",
       "\n",
       "   message_len  \n",
       "0          111  \n",
       "1           29  \n",
       "2          155  \n",
       "3           49  \n",
       "4           61  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['message_len']=sms.message.apply(len)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e74cf87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Message Length')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sms[sms.label=='ham'].message_len.plot(bins=35,kind='hist',color='blue',label='ham messages',alpha=0.6)\n",
    "sms[sms.label=='spam'].message_len.plot(kind='hist',color='red',label='spam message',alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel('Message Length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6509a",
   "metadata": {},
   "source": [
    "Very interesting! Through just basic EDA we've been able to discover a trend that spam messages tend to have more characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b2910b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4825.0</td>\n",
       "      <td>4825.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>71.023627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.016023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>910.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label_num  message_len\n",
       "count     4825.0  4825.000000\n",
       "mean         0.0    71.023627\n",
       "std          0.0    58.016023\n",
       "min          0.0     2.000000\n",
       "25%          0.0    33.000000\n",
       "50%          0.0    52.000000\n",
       "75%          0.0    92.000000\n",
       "max          0.0   910.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms.label=='ham'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be55fd1",
   "metadata": {},
   "source": [
    "Woah! 910 characters, let's use masking to find this message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e75624b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms.message_len==910].message.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcd5484e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>747.0</td>\n",
       "      <td>747.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>138.866131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>29.183082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>132.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>149.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>224.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label_num  message_len\n",
       "count      747.0   747.000000\n",
       "mean         1.0   138.866131\n",
       "std          0.0    29.183082\n",
       "min          1.0    13.000000\n",
       "25%          1.0   132.500000\n",
       "50%          1.0   149.000000\n",
       "75%          1.0   157.000000\n",
       "max          1.0   224.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms.label=='spam'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6e5a0",
   "metadata": {},
   "source": [
    "### üìë Text Pre-processing\n",
    "\n",
    "- Our main issue with our data is that it is all in text format (strings). The classification algorithms that we usally use need some sort of numerical feature vector in order to perform the classification task. There are actually many methods to convert a corpus(set of words) to a vector format. The simplest is the bag-of-words approach, where each unique word in a text will be represented by one number.\n",
    "\n",
    "- In this section we'll convert the raw messages (sequence of characters) into vectors (sequences of numbers).\n",
    "\n",
    "- As a first step, let's write a function that will split a message into its individual words and return a list. We'll also remove very common words, ('the', 'a', etc..). To do this we will take advantage of the NLTK library. It's pretty much the standard library in Python for processing text and has a lot of useful features. We'll only use some of the basic ones here.\n",
    "\n",
    "- Let's create a function that will process the string in the message column, then we can just use apply() in pandas do process all the text in the DataFrame.\n",
    "\n",
    "- First removing punctuation. We can just take advantage of Python's built-in string library to get a quick list of all the possible punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c9ff9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    STOPWORDS=stopwords.words('english')+['u','√º', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "    \n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc=[char for char in mess if char not in string.punctuation]\n",
    "    \n",
    "    # Join the characters again to form the string.\n",
    "    nopunc=''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce901388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c333bac7",
   "metadata": {},
   "source": [
    "Certainly! Here's a detailed explanation of each line of code:\n",
    "\n",
    "```python\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "```\n",
    "These lines import the necessary modules for the code. The `string` module provides a set of constants containing common punctuation characters, and the `stopwords` module from NLTK's corpus provides a list of common English stopwords.\n",
    "\n",
    "```python\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    STOPWORDS=stopwords.words('english')+['u','√º', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "```\n",
    "This line defines a function called `text_process` that takes in a string `mess` as a parameter. The function's docstring explains what the function does: it removes punctuation and stopwords from the input text and returns a list of cleaned text.\n",
    "\n",
    "The `STOPWORDS` variable is defined as a list. It contains the stopwords from the NLTK corpus obtained by calling `stopwords.words('english')`, and additional custom stopwords such as 'u', '√º', 'ur', '4', '2', 'im', 'dont', 'doin', and 'ure'.\n",
    "\n",
    "```python\n",
    "    nopunc=[char for char in mess if char not in string.punctuation]\n",
    "```\n",
    "This line uses a list comprehension to create a new list called `nopunc`. It iterates over each character (`char`) in the `mess` string, and checks if the character is not present in the `string.punctuation` list. If the character is not a punctuation character, it is added to the `nopunc` list.\n",
    "\n",
    "```python\n",
    "    nopunc=''.join(nopunc)\n",
    "```\n",
    "This line joins the characters in the `nopunc` list back into a single string without any punctuation. The `join` method is called on an empty string `''`, and it concatenates each character in the `nopunc` list together.\n",
    "\n",
    "```python\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])\n",
    "```\n",
    "This line performs the final cleaning step. It splits the `nopunc` string into a list of words using the `split` method, which splits the string at whitespace characters. Then, it uses a list comprehension to create a new list called `cleaned` by iterating over each word in the `nopunc` list. It checks if the lowercase version of the word (`word.lower()`) is not present in the `STOPWORDS` list. If the word is not a stopword, it is included in the `cleaned` list.\n",
    "\n",
    "Finally, the `join` method is used to join the words in the `cleaned` list back into a single string, with each word separated by a space. This cleaned text string is then returned by the function.\n",
    "\n",
    "I hope this explanation helps you understand each line of the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e571a",
   "metadata": {},
   "source": [
    "**do give me the basic example as output of those above operations**\n",
    "\n",
    "Certainly! Let's consider an example to see the output of the code. Suppose we have the following input string:\n",
    "\n",
    "```python\n",
    "input_text = \"Hello, this is a sample text! It contains some punctuation marks, and stopwords as well.\"\n",
    "```\n",
    "\n",
    "Now, let's apply the `text_process` function to this input and see the output step by step:\n",
    "\n",
    "1. Remove all punctuation:\n",
    "   The punctuation characters in the input text are: `,`, `!`, `.`.\n",
    "   After removing the punctuation, the `nopunc` string becomes: \"Hello this is a sample text It contains some punctuation marks and stopwords as well\".\n",
    "\n",
    "2. Remove all stopwords:\n",
    "   The stopwords in the `STOPWORDS` list are: `u`, `√º`, `ur`, `4`, `2`, `im`, `dont`, `doin`, `ure`.\n",
    "   After removing the stopwords, the cleaned text becomes: \"Hello sample text contains punctuation marks stopwords well\".\n",
    "\n",
    "The final cleaned text is returned as a string.\n",
    "\n",
    "So, if we apply the `text_process` function to the input text, the output will be:\n",
    "```python\n",
    "output_text = \"Hello sample text contains punctuation marks stopwords well\"\n",
    "```\n",
    "\n",
    "This is the result of removing punctuation and stopwords from the input string using the `text_process` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bf490c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0   \n",
       "1   ham                      Ok lar... Joking wif u oni...          0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1   \n",
       "3   ham  U dun say so early hor... U c already then say...          0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0   \n",
       "\n",
       "   message_len  \n",
       "0          111  \n",
       "1           29  \n",
       "2          155  \n",
       "3           49  \n",
       "4           61  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b699a",
   "metadata": {},
   "source": [
    "Now let's \"tokenize\" these messages. Tokenization is just the term used to describe the process of converting the normal text strings in to a list of tokens (words that we actually want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6afcc585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "      <th>clean_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>Go jurong point crazy Available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>Ok lar Joking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>Free entry wkly comp win FA Cup final tkts 21s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>dun say early hor c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>Nah think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0   \n",
       "1   ham                      Ok lar... Joking wif u oni...          0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1   \n",
       "3   ham  U dun say so early hor... U c already then say...          0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0   \n",
       "\n",
       "   message_len                                          clean_msg  \n",
       "0          111  Go jurong point crazy Available bugis n great ...  \n",
       "1           29                              Ok lar Joking wif oni  \n",
       "2          155  Free entry wkly comp win FA Cup final tkts 21s...  \n",
       "3           49                    dun say early hor c already say  \n",
       "4           61             Nah think goes usf lives around though  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['clean_msg']=sms.message.apply(text_process)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d05364c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab946ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('get', 303), ('ltgt', 276), ('ok', 272), ('go', 247), ('ill', 236), ('know', 232), ('got', 231), ('like', 229), ('call', 229), ('come', 224), ('good', 222), ('time', 189), ('day', 187), ('love', 185), ('going', 167), ('want', 163), ('one', 162), ('home', 160), ('lor', 160), ('need', 156), ('sorry', 153), ('still', 146), ('see', 137), ('n', 134), ('later', 134), ('da', 131), ('r', 131), ('back', 129), ('think', 128), ('well', 126), ('today', 125), ('send', 123), ('tell', 121), ('cant', 118), ('√¨', 117), ('hi', 117), ('take', 112), ('much', 112), ('oh', 111), ('night', 107), ('hey', 106), ('happy', 105), ('great', 100), ('way', 100), ('hope', 99), ('pls', 98), ('work', 96), ('wat', 95), ('thats', 94), ('dear', 94)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words=sms[sms.label=='ham'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])\n",
    "ham_words=Counter()\n",
    "\n",
    "for msg in words:\n",
    "    ham_words.update(msg)\n",
    "    \n",
    "print(ham_words.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43bffa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'go': 247,\n",
       "         'jurong': 1,\n",
       "         'point': 13,\n",
       "         'crazy': 10,\n",
       "         'available': 13,\n",
       "         'bugis': 7,\n",
       "         'n': 134,\n",
       "         'great': 100,\n",
       "         'world': 32,\n",
       "         'la': 7,\n",
       "         'e': 78,\n",
       "         'buffet': 2,\n",
       "         'cine': 7,\n",
       "         'got': 231,\n",
       "         'amore': 1,\n",
       "         'wat': 95,\n",
       "         'ok': 272,\n",
       "         'lar': 38,\n",
       "         'joking': 6,\n",
       "         'wif': 27,\n",
       "         'oni': 4,\n",
       "         'dun': 55,\n",
       "         'say': 91,\n",
       "         'early': 33,\n",
       "         'hor': 2,\n",
       "         'c': 60,\n",
       "         'already': 89,\n",
       "         'nah': 10,\n",
       "         'think': 128,\n",
       "         'goes': 26,\n",
       "         'usf': 11,\n",
       "         'lives': 4,\n",
       "         'around': 59,\n",
       "         'though': 26,\n",
       "         'even': 55,\n",
       "         'brother': 18,\n",
       "         'like': 229,\n",
       "         'speak': 23,\n",
       "         'treat': 16,\n",
       "         'aids': 1,\n",
       "         'patent': 1,\n",
       "         'per': 12,\n",
       "         'request': 7,\n",
       "         'melle': 6,\n",
       "         'oru': 4,\n",
       "         'minnaminunginte': 3,\n",
       "         'nurungu': 3,\n",
       "         'vettam': 3,\n",
       "         'set': 19,\n",
       "         'callertune': 10,\n",
       "         'callers': 5,\n",
       "         'press': 7,\n",
       "         '9': 26,\n",
       "         'copy': 9,\n",
       "         'friends': 53,\n",
       "         'gonna': 58,\n",
       "         'home': 160,\n",
       "         'soon': 56,\n",
       "         'want': 163,\n",
       "         'talk': 40,\n",
       "         'stuff': 42,\n",
       "         'anymore': 8,\n",
       "         'tonight': 57,\n",
       "         'k': 71,\n",
       "         'ive': 73,\n",
       "         'cried': 1,\n",
       "         'enough': 27,\n",
       "         'today': 125,\n",
       "         'searching': 5,\n",
       "         'right': 89,\n",
       "         'words': 22,\n",
       "         'thank': 27,\n",
       "         'breather': 1,\n",
       "         'promise': 9,\n",
       "         'wont': 51,\n",
       "         'take': 112,\n",
       "         'help': 36,\n",
       "         'granted': 1,\n",
       "         'fulfil': 1,\n",
       "         'wonderful': 15,\n",
       "         'blessing': 2,\n",
       "         'times': 24,\n",
       "         'date': 9,\n",
       "         'sunday': 8,\n",
       "         'oh': 111,\n",
       "         'kim': 1,\n",
       "         'watching': 34,\n",
       "         'eh': 12,\n",
       "         'remember': 31,\n",
       "         'spell': 3,\n",
       "         'name': 34,\n",
       "         'yes': 71,\n",
       "         'v': 47,\n",
       "         'naughty': 6,\n",
       "         'make': 88,\n",
       "         'wet': 3,\n",
       "         'fine': 45,\n",
       "         'that√•√µs': 4,\n",
       "         'way': 100,\n",
       "         'feel': 62,\n",
       "         'gota': 1,\n",
       "         'b': 54,\n",
       "         'seriously': 9,\n",
       "         'i\\x89√ª√∑m': 5,\n",
       "         'going': 167,\n",
       "         'try': 42,\n",
       "         'months': 9,\n",
       "         'ha': 16,\n",
       "         '√¨': 117,\n",
       "         'pay': 30,\n",
       "         'first': 56,\n",
       "         'da': 131,\n",
       "         'stock': 5,\n",
       "         'comin': 11,\n",
       "         'aft': 19,\n",
       "         'finish': 42,\n",
       "         'lunch': 45,\n",
       "         'str': 3,\n",
       "         'lor': 160,\n",
       "         'ard': 21,\n",
       "         '3': 44,\n",
       "         'smth': 16,\n",
       "         'ffffffffff': 1,\n",
       "         'alright': 23,\n",
       "         'meet': 72,\n",
       "         'sooner': 4,\n",
       "         'forced': 1,\n",
       "         'eat': 38,\n",
       "         'slice': 2,\n",
       "         'really': 85,\n",
       "         'hungry': 12,\n",
       "         'tho': 17,\n",
       "         'sucks': 7,\n",
       "         'mark': 8,\n",
       "         'getting': 46,\n",
       "         'worried': 9,\n",
       "         'knows': 9,\n",
       "         'sick': 9,\n",
       "         'turn': 3,\n",
       "         'pizza': 8,\n",
       "         'lol': 73,\n",
       "         'always': 56,\n",
       "         'convincing': 1,\n",
       "         'catch': 9,\n",
       "         'bus': 29,\n",
       "         'frying': 3,\n",
       "         'egg': 4,\n",
       "         'tea': 6,\n",
       "         'eating': 7,\n",
       "         'moms': 9,\n",
       "         'left': 29,\n",
       "         'dinner': 35,\n",
       "         'love': 185,\n",
       "         'back': 129,\n",
       "         'amp': 82,\n",
       "         'packing': 1,\n",
       "         'car': 39,\n",
       "         'ill': 236,\n",
       "         'let': 70,\n",
       "         'know': 232,\n",
       "         'theres': 25,\n",
       "         'room': 34,\n",
       "         'ahhh': 1,\n",
       "         'work': 96,\n",
       "         'vaguely': 1,\n",
       "         'wait': 57,\n",
       "         'thats': 94,\n",
       "         'still': 146,\n",
       "         'clear': 3,\n",
       "         'sure': 71,\n",
       "         'sarcastic': 2,\n",
       "         'x': 39,\n",
       "         'doesnt': 26,\n",
       "         'live': 17,\n",
       "         'us': 55,\n",
       "         'yeah': 85,\n",
       "         'apologetic': 1,\n",
       "         'fallen': 1,\n",
       "         'actin': 1,\n",
       "         'spoilt': 1,\n",
       "         'child': 2,\n",
       "         'caught': 3,\n",
       "         'till': 21,\n",
       "         'badly': 1,\n",
       "         'cheers': 8,\n",
       "         'tell': 121,\n",
       "         'anything': 73,\n",
       "         'fear': 3,\n",
       "         'fainting': 1,\n",
       "         'housework': 1,\n",
       "         'quick': 8,\n",
       "         'cuppa': 1,\n",
       "         'yup': 43,\n",
       "         'look': 28,\n",
       "         'timings': 1,\n",
       "         'msg': 54,\n",
       "         'xuhui': 3,\n",
       "         'learn': 5,\n",
       "         '2nd': 10,\n",
       "         'may': 39,\n",
       "         'lesson': 17,\n",
       "         '8am': 2,\n",
       "         'oops': 10,\n",
       "         'roommates': 5,\n",
       "         'done': 45,\n",
       "         'see': 137,\n",
       "         'letter': 4,\n",
       "         'decide': 9,\n",
       "         'hello': 46,\n",
       "         'hows': 44,\n",
       "         'saturday': 9,\n",
       "         'texting': 5,\n",
       "         'youd': 8,\n",
       "         'decided': 16,\n",
       "         'tomo': 17,\n",
       "         'trying': 28,\n",
       "         'invite': 3,\n",
       "         'pls': 98,\n",
       "         'ahead': 7,\n",
       "         'watts': 1,\n",
       "         'wanted': 26,\n",
       "         'weekend': 24,\n",
       "         'abiola': 11,\n",
       "         'forget': 16,\n",
       "         'need': 156,\n",
       "         'crave': 11,\n",
       "         'sweet': 33,\n",
       "         'arabian': 1,\n",
       "         'steed': 1,\n",
       "         'mmmmmm': 2,\n",
       "         'yummy': 3,\n",
       "         'seeing': 14,\n",
       "         'hope': 99,\n",
       "         'man': 44,\n",
       "         'well': 126,\n",
       "         'endowed': 1,\n",
       "         'ltgt': 276,\n",
       "         'inches': 4,\n",
       "         'callsmessagesmissed': 3,\n",
       "         'calls': 12,\n",
       "         'didnt': 79,\n",
       "         'get': 303,\n",
       "         'hep': 1,\n",
       "         'immunisation': 1,\n",
       "         'nigeria': 8,\n",
       "         'fair': 3,\n",
       "         'hopefully': 5,\n",
       "         'tyler': 5,\n",
       "         'cant': 118,\n",
       "         'could': 53,\n",
       "         'maybe': 29,\n",
       "         'ask': 88,\n",
       "         'bit': 43,\n",
       "         'stubborn': 1,\n",
       "         'hospital': 10,\n",
       "         'kept': 4,\n",
       "         'telling': 14,\n",
       "         'weak': 5,\n",
       "         'sucker': 1,\n",
       "         'hospitals': 2,\n",
       "         'suckers': 1,\n",
       "         'thinked': 1,\n",
       "         'time': 189,\n",
       "         'saw': 23,\n",
       "         'class': 43,\n",
       "         'gram': 3,\n",
       "         'usually': 6,\n",
       "         'runs': 3,\n",
       "         'half': 25,\n",
       "         'eighth': 2,\n",
       "         'smarter': 1,\n",
       "         'gets': 11,\n",
       "         'almost': 13,\n",
       "         'whole': 14,\n",
       "         'second': 18,\n",
       "         'fyi': 6,\n",
       "         'ride': 5,\n",
       "         'tomorrow': 80,\n",
       "         'morning': 77,\n",
       "         'hes': 32,\n",
       "         'crashing': 1,\n",
       "         'place': 54,\n",
       "         'wow': 10,\n",
       "         'never': 42,\n",
       "         'realized': 3,\n",
       "         'embarassed': 4,\n",
       "         'accomodations': 1,\n",
       "         'thought': 46,\n",
       "         'liked': 7,\n",
       "         'since': 25,\n",
       "         'best': 35,\n",
       "         'seemed': 4,\n",
       "         'happy': 105,\n",
       "         'cave': 1,\n",
       "         'sorry': 153,\n",
       "         'give': 92,\n",
       "         'offered': 1,\n",
       "         'embarassing': 1,\n",
       "         'mallika': 1,\n",
       "         'sherawat': 1,\n",
       "         'yesterday': 20,\n",
       "         'find': 50,\n",
       "         'lturlgt': 3,\n",
       "         'call': 229,\n",
       "         'later': 134,\n",
       "         'meeting': 43,\n",
       "         'reached': 10,\n",
       "         'yesgauti': 1,\n",
       "         'sehwag': 1,\n",
       "         'odi': 2,\n",
       "         'series': 4,\n",
       "         'pick': 69,\n",
       "         '1': 49,\n",
       "         'burger': 1,\n",
       "         'move': 11,\n",
       "         'pain': 24,\n",
       "         'killing': 2,\n",
       "         'good': 222,\n",
       "         'joke': 5,\n",
       "         'girls': 11,\n",
       "         'situation': 9,\n",
       "         'seekers': 1,\n",
       "         'part': 21,\n",
       "         'checking': 11,\n",
       "         'iq': 4,\n",
       "         'took': 19,\n",
       "         'forever': 9,\n",
       "         'come': 224,\n",
       "         'double': 5,\n",
       "         'check': 34,\n",
       "         'hair': 24,\n",
       "         'dresser': 3,\n",
       "         'said': 86,\n",
       "         'wun': 7,\n",
       "         'cut': 13,\n",
       "         'short': 9,\n",
       "         'nice': 57,\n",
       "         'song': 9,\n",
       "         'dedicated': 2,\n",
       "         'day': 187,\n",
       "         'dedicate': 2,\n",
       "         'send': 123,\n",
       "         'valuable': 4,\n",
       "         'frnds': 14,\n",
       "         'rply': 5,\n",
       "         'plane': 5,\n",
       "         'month': 28,\n",
       "         'end': 26,\n",
       "         'wah': 4,\n",
       "         'lucky': 9,\n",
       "         'save': 7,\n",
       "         'money': 53,\n",
       "         'hee': 13,\n",
       "         'finished': 19,\n",
       "         'hi': 117,\n",
       "         'babe': 70,\n",
       "         'wanna': 28,\n",
       "         'something': 68,\n",
       "         'xx': 7,\n",
       "         'kkwhere': 2,\n",
       "         'youhow': 1,\n",
       "         'performed': 1,\n",
       "         'waiting': 36,\n",
       "         'machan': 4,\n",
       "         'free': 59,\n",
       "         'cool': 40,\n",
       "         'gentleman': 3,\n",
       "         'dignity': 3,\n",
       "         'respect': 3,\n",
       "         'peoples': 1,\n",
       "         'much': 112,\n",
       "         'shy': 1,\n",
       "         'pa': 32,\n",
       "         'operate': 1,\n",
       "         'looking': 20,\n",
       "         'job': 43,\n",
       "         'tas': 1,\n",
       "         'earn': 2,\n",
       "         'ah': 30,\n",
       "         'next': 46,\n",
       "         'stop': 37,\n",
       "         'network': 4,\n",
       "         'urgnt': 5,\n",
       "         'sms': 18,\n",
       "         'real': 24,\n",
       "         'yo': 36,\n",
       "         'tickets': 7,\n",
       "         'one': 162,\n",
       "         'jacket': 2,\n",
       "         'used': 14,\n",
       "         'multis': 1,\n",
       "         'started': 17,\n",
       "         'requests': 2,\n",
       "         'came': 26,\n",
       "         'bed': 32,\n",
       "         'coins': 4,\n",
       "         'factory': 1,\n",
       "         'gotta': 12,\n",
       "         'cash': 12,\n",
       "         'nitros': 2,\n",
       "         'ela': 2,\n",
       "         'kanoil': 1,\n",
       "         'download': 10,\n",
       "         'wen': 24,\n",
       "         'don\\x89√ª√∑t': 6,\n",
       "         'stand': 7,\n",
       "         'close': 14,\n",
       "         'you\\x89√ª√∑ll': 1,\n",
       "         'another': 35,\n",
       "         'night': 107,\n",
       "         'spent': 8,\n",
       "         'late': 55,\n",
       "         'afternoon': 27,\n",
       "         'casualty': 1,\n",
       "         'means': 26,\n",
       "         'havent': 45,\n",
       "         'stuff42moro': 1,\n",
       "         'includes': 1,\n",
       "         'sheets': 3,\n",
       "         'smile': 44,\n",
       "         'pleasure': 7,\n",
       "         'trouble': 8,\n",
       "         'pours': 2,\n",
       "         'rain': 9,\n",
       "         'sum1': 3,\n",
       "         'hurts': 8,\n",
       "         'becoz': 4,\n",
       "         'someone': 45,\n",
       "         'loves': 8,\n",
       "         'smiling': 20,\n",
       "         'planning': 8,\n",
       "         'buy': 62,\n",
       "         'lido': 3,\n",
       "         '530': 7,\n",
       "         'show': 21,\n",
       "         'telugu': 1,\n",
       "         'moviewat': 1,\n",
       "         'abt': 26,\n",
       "         'loads': 10,\n",
       "         'loans': 1,\n",
       "         'wk': 7,\n",
       "         'hols': 2,\n",
       "         'run': 23,\n",
       "         'forgot': 31,\n",
       "         'hairdressers': 1,\n",
       "         'appointment': 4,\n",
       "         'four': 2,\n",
       "         'shower': 11,\n",
       "         'beforehand': 1,\n",
       "         'cause': 18,\n",
       "         'prob': 16,\n",
       "         'ham': 3,\n",
       "         'please': 76,\n",
       "         'text': 68,\n",
       "         'nothing': 34,\n",
       "         'else': 23,\n",
       "         'okay': 26,\n",
       "         'price': 7,\n",
       "         'long': 42,\n",
       "         'legal': 4,\n",
       "         'ave': 5,\n",
       "         'ams': 1,\n",
       "         'gone': 15,\n",
       "         '4the': 1,\n",
       "         'driving': 20,\n",
       "         'test': 22,\n",
       "         'yet': 48,\n",
       "         'youre': 51,\n",
       "         'mean': 20,\n",
       "         'guess': 31,\n",
       "         'gave': 10,\n",
       "         'boston': 5,\n",
       "         'men': 7,\n",
       "         'changed': 6,\n",
       "         'search': 15,\n",
       "         'location': 2,\n",
       "         'nyc': 4,\n",
       "         'cuz': 7,\n",
       "         'signin': 1,\n",
       "         'page': 6,\n",
       "         'says': 27,\n",
       "         'umma': 7,\n",
       "         'life': 74,\n",
       "         'vava': 3,\n",
       "         'lot': 34,\n",
       "         'dear': 94,\n",
       "         'thanks': 60,\n",
       "         'wishes': 8,\n",
       "         'birthday': 35,\n",
       "         'making': 23,\n",
       "         'truly': 2,\n",
       "         'memorable': 1,\n",
       "         'aight': 33,\n",
       "         'hit': 11,\n",
       "         'would': 72,\n",
       "         'ip': 1,\n",
       "         'address': 17,\n",
       "         'considering': 4,\n",
       "         'computer': 7,\n",
       "         'isnt': 17,\n",
       "         'minecraft': 1,\n",
       "         'server': 1,\n",
       "         'grumpy': 1,\n",
       "         'old': 18,\n",
       "         'people': 47,\n",
       "         'mom': 12,\n",
       "         'better': 41,\n",
       "         'lying': 1,\n",
       "         'play': 20,\n",
       "         'jokes': 5,\n",
       "         'worry': 22,\n",
       "         'busy': 17,\n",
       "         'plural': 1,\n",
       "         'noun': 2,\n",
       "         'research': 4,\n",
       "         'dinnermsg': 1,\n",
       "         'cos': 76,\n",
       "         'new': 67,\n",
       "         'things': 51,\n",
       "         'scared': 5,\n",
       "         'mah': 13,\n",
       "         'loud': 3,\n",
       "         'wa': 3,\n",
       "         'openin': 1,\n",
       "         'sentence': 3,\n",
       "         'formal': 1,\n",
       "         'anyway': 27,\n",
       "         'juz': 25,\n",
       "         'tt': 4,\n",
       "         'eatin': 7,\n",
       "         'puttin': 2,\n",
       "         'weighthaha': 1,\n",
       "         'anythin': 3,\n",
       "         'special': 32,\n",
       "         'happened': 16,\n",
       "         'entered': 5,\n",
       "         'cabin': 5,\n",
       "         'bday': 15,\n",
       "         'boss': 9,\n",
       "         'felt': 13,\n",
       "         'askd': 12,\n",
       "         'invited': 8,\n",
       "         'apartment': 8,\n",
       "         'went': 61,\n",
       "         'goodo': 2,\n",
       "         'must': 22,\n",
       "         'friday': 14,\n",
       "         'eggpotato': 1,\n",
       "         'ratio': 1,\n",
       "         'tortilla': 2,\n",
       "         'needed': 3,\n",
       "         'hmmmy': 1,\n",
       "         'uncle': 12,\n",
       "         'informed': 3,\n",
       "         'paying': 7,\n",
       "         'school': 27,\n",
       "         'directly': 6,\n",
       "         'food': 23,\n",
       "         'applespairsall': 1,\n",
       "         'malarky': 1,\n",
       "         'sao': 1,\n",
       "         'mu': 14,\n",
       "         '12': 4,\n",
       "         '√¨√Ø': 52,\n",
       "         'predict': 1,\n",
       "         '√¨ll': 2,\n",
       "         'buying': 9,\n",
       "         'knowyetunde': 1,\n",
       "         'hasnt': 5,\n",
       "         'sent': 58,\n",
       "         'bother': 7,\n",
       "         'sending': 10,\n",
       "         'involve': 1,\n",
       "         'shouldnt': 5,\n",
       "         'imposed': 1,\n",
       "         'apologise': 3,\n",
       "         'hey': 106,\n",
       "         'girl': 29,\n",
       "         'r': 131,\n",
       "         'del': 1,\n",
       "         'bak': 8,\n",
       "         'sum': 2,\n",
       "         'lucyxx': 1,\n",
       "         'kkhow': 4,\n",
       "         'cost': 10,\n",
       "         'tmorrowpls': 1,\n",
       "         'accomodate': 1,\n",
       "         'answer': 14,\n",
       "         'question': 15,\n",
       "         'haf': 23,\n",
       "         'msn': 2,\n",
       "         'yijuehotmailcom': 1,\n",
       "         'rooms': 4,\n",
       "         'befor': 4,\n",
       "         'activities': 4,\n",
       "         'lazy': 9,\n",
       "         'type': 15,\n",
       "         'lect': 9,\n",
       "         'pouch': 2,\n",
       "         'sir': 33,\n",
       "         'mail': 24,\n",
       "         'swt': 2,\n",
       "         'nver': 1,\n",
       "         'tired': 12,\n",
       "         'little': 27,\n",
       "         'lovable': 10,\n",
       "         'persons': 7,\n",
       "         'cozsomtimes': 1,\n",
       "         'occupy': 2,\n",
       "         'biggest': 1,\n",
       "         'hearts': 1,\n",
       "         'gud': 53,\n",
       "         'ni8': 12,\n",
       "         'open': 16,\n",
       "         'ya': 55,\n",
       "         'dot': 1,\n",
       "         'whats': 32,\n",
       "         'staff': 2,\n",
       "         'taking': 17,\n",
       "         'ummmawill': 1,\n",
       "         'inour': 1,\n",
       "         'begin': 4,\n",
       "         'qatar': 4,\n",
       "         'pray': 8,\n",
       "         'hard': 13,\n",
       "         'ki': 6,\n",
       "         'deleted': 4,\n",
       "         'contact': 12,\n",
       "         'sindu': 1,\n",
       "         'birla': 3,\n",
       "         'soft': 3,\n",
       "         'wine': 11,\n",
       "         'flowing': 2,\n",
       "         'nevering': 1,\n",
       "         'thk': 50,\n",
       "         'plaza': 2,\n",
       "         'typical': 1,\n",
       "         'reply': 43,\n",
       "         'everywhere': 2,\n",
       "         'dirt': 1,\n",
       "         'floor': 3,\n",
       "         'windows': 2,\n",
       "         'shirt': 4,\n",
       "         'sometimes': 8,\n",
       "         'mouth': 2,\n",
       "         'comes': 18,\n",
       "         'dream': 7,\n",
       "         'without': 28,\n",
       "         'chores': 1,\n",
       "         'joy': 4,\n",
       "         'lots': 15,\n",
       "         'tv': 22,\n",
       "         'shows': 5,\n",
       "         'exist': 1,\n",
       "         'hail': 1,\n",
       "         'mist': 1,\n",
       "         'become': 5,\n",
       "         'aaooooright': 1,\n",
       "         'leaving': 19,\n",
       "         'house': 35,\n",
       "         'interview': 4,\n",
       "         'boy': 30,\n",
       "         'meare': 1,\n",
       "         'missing': 26,\n",
       "         'keep': 62,\n",
       "         'safe': 11,\n",
       "         'miss': 69,\n",
       "         'envy': 1,\n",
       "         'everyone': 16,\n",
       "         'sees': 1,\n",
       "         'parentsi': 1,\n",
       "         'hand': 15,\n",
       "         'excited': 1,\n",
       "         'spend': 10,\n",
       "         'points': 4,\n",
       "         'cultures': 1,\n",
       "         'module': 2,\n",
       "         'frnd': 10,\n",
       "         'avoid': 2,\n",
       "         'missunderstding': 1,\n",
       "         'wit': 15,\n",
       "         'beloved': 2,\n",
       "         'ones': 8,\n",
       "         'escape': 4,\n",
       "         'fancy': 5,\n",
       "         'bridge': 1,\n",
       "         'needs': 15,\n",
       "         'lager': 1,\n",
       "         'completely': 8,\n",
       "         'formclark': 2,\n",
       "         'also': 68,\n",
       "         'utter': 2,\n",
       "         'waste': 7,\n",
       "         'axis': 1,\n",
       "         'bank': 12,\n",
       "         'account': 20,\n",
       "         'hmmm': 9,\n",
       "         'hop': 7,\n",
       "         'muz': 11,\n",
       "         'discuss': 5,\n",
       "         'liao': 35,\n",
       "         'coming': 50,\n",
       "         'bloody': 4,\n",
       "         'hell': 12,\n",
       "         'believe': 18,\n",
       "         'surname': 1,\n",
       "         'mr': 10,\n",
       "         'clue': 1,\n",
       "         'spanish': 1,\n",
       "         'begins': 1,\n",
       "         'bath': 11,\n",
       "         'goodfine': 1,\n",
       "         'youve': 10,\n",
       "         'carlos': 15,\n",
       "         'mall': 3,\n",
       "         'turns': 6,\n",
       "         'staying': 9,\n",
       "         'til': 25,\n",
       "         'smoke': 17,\n",
       "         'worth': 4,\n",
       "         'log': 7,\n",
       "         'spoke': 6,\n",
       "         'maneesha': 3,\n",
       "         'wed': 9,\n",
       "         'satisfied': 3,\n",
       "         'experience': 7,\n",
       "         'toll': 3,\n",
       "         'lifted': 1,\n",
       "         'hopes': 1,\n",
       "         'offer': 7,\n",
       "         'especially': 9,\n",
       "         'approaches': 1,\n",
       "         'studying': 9,\n",
       "         'anyways': 4,\n",
       "         'gr8': 12,\n",
       "         'trust': 9,\n",
       "         'guys': 32,\n",
       "         'greatbye': 1,\n",
       "         'handsome': 1,\n",
       "         'finding': 1,\n",
       "         'working': 29,\n",
       "         'towards': 7,\n",
       "         'net': 7,\n",
       "         'mummy': 3,\n",
       "         'wheres': 8,\n",
       "         'boytoy': 16,\n",
       "         'haha': 48,\n",
       "         'awesome': 19,\n",
       "         'minute': 12,\n",
       "         'xmas': 13,\n",
       "         'radio': 3,\n",
       "         'jus': 35,\n",
       "         'bathe': 10,\n",
       "         'sis': 17,\n",
       "         'using': 7,\n",
       "         'finishes': 3,\n",
       "         'joined': 4,\n",
       "         'league': 1,\n",
       "         'touch': 16,\n",
       "         'deal': 8,\n",
       "         'friend': 36,\n",
       "         'personal': 4,\n",
       "         'week': 66,\n",
       "         'finally': 14,\n",
       "         'completed': 2,\n",
       "         'course': 14,\n",
       "         'however': 7,\n",
       "         'suggest': 3,\n",
       "         'stays': 2,\n",
       "         'able': 26,\n",
       "         'ors': 1,\n",
       "         'every': 39,\n",
       "         'stool': 1,\n",
       "         'settled': 4,\n",
       "         'year': 44,\n",
       "         'wishin': 2,\n",
       "         'mrng': 14,\n",
       "         'hav': 26,\n",
       "         'story': 20,\n",
       "         'hamster': 2,\n",
       "         'dead': 8,\n",
       "         'tmr': 28,\n",
       "         '1pm': 1,\n",
       "         'orchard': 11,\n",
       "         'mrt': 9,\n",
       "         'kate': 11,\n",
       "         'evening': 26,\n",
       "         'babyjontet': 1,\n",
       "         'txt': 13,\n",
       "         'xxx': 21,\n",
       "         'found': 16,\n",
       "         'enc': 1,\n",
       "         'bucks': 7,\n",
       "         'darlin': 14,\n",
       "         'college': 15,\n",
       "         'refilled': 2,\n",
       "         'successfully': 3,\n",
       "         'inr': 2,\n",
       "         'ltdecimalgt': 23,\n",
       "         'keralacircle': 2,\n",
       "         'prepaid': 2,\n",
       "         'balance': 4,\n",
       "         'rs': 10,\n",
       "         'transaction': 5,\n",
       "         'id': 29,\n",
       "         'kr': 2,\n",
       "         'goodmorning': 14,\n",
       "         'sleeping': 17,\n",
       "         'ga': 1,\n",
       "         'alter': 1,\n",
       "         '11': 4,\n",
       "         'dat': 39,\n",
       "         'ericsson': 2,\n",
       "         'oso': 23,\n",
       "         'cannot': 5,\n",
       "         'oredi': 15,\n",
       "         'dats': 3,\n",
       "         'straight': 4,\n",
       "         'dogg': 1,\n",
       "         'connection': 4,\n",
       "         'refund': 1,\n",
       "         'bill': 7,\n",
       "         'shoot': 4,\n",
       "         'big': 35,\n",
       "         'ready': 35,\n",
       "         'bruv': 2,\n",
       "         'break': 13,\n",
       "         'rewarding': 2,\n",
       "         'semester': 12,\n",
       "         'chat': 14,\n",
       "         'kkgoodstudy': 1,\n",
       "         'noe': 20,\n",
       "         'leh': 29,\n",
       "         'sounds': 15,\n",
       "         'match': 5,\n",
       "         'heading': 2,\n",
       "         'draw': 5,\n",
       "         'prediction': 1,\n",
       "         'slept': 8,\n",
       "         'past': 7,\n",
       "         'nights': 3,\n",
       "         'easy': 18,\n",
       "         'ahsen': 2,\n",
       "         'selected': 4,\n",
       "         'exam': 11,\n",
       "         'march': 13,\n",
       "         'use': 36,\n",
       "         'gt': 8,\n",
       "         'atm': 3,\n",
       "         'register': 1,\n",
       "         'os': 2,\n",
       "         'called': 32,\n",
       "         'ubandu': 1,\n",
       "         'installing': 2,\n",
       "         'diskyou': 1,\n",
       "         'important': 14,\n",
       "         'files': 4,\n",
       "         'system': 6,\n",
       "         'repair': 2,\n",
       "         'shop': 13,\n",
       "         'happen': 20,\n",
       "         'romantic': 2,\n",
       "         'nite': 21,\n",
       "         'scenery': 1,\n",
       "         'appreciate': 7,\n",
       "         'company': 15,\n",
       "         'elama': 1,\n",
       "         'po': 3,\n",
       "         'mudyadhu': 1,\n",
       "         'strict': 1,\n",
       "         'teacher': 3,\n",
       "         'bcoz': 10,\n",
       "         'teaches': 4,\n",
       "         'conducts': 2,\n",
       "         'lessons': 9,\n",
       "         'gandhipuram': 1,\n",
       "         'walk': 18,\n",
       "         'cross': 1,\n",
       "         'road': 8,\n",
       "         'side': 13,\n",
       "         'street': 6,\n",
       "         'rubber': 1,\n",
       "         'battery': 7,\n",
       "         'died': 5,\n",
       "         'yeshere': 2,\n",
       "         'printed': 2,\n",
       "         'upstairs': 2,\n",
       "         'closer': 4,\n",
       "         'youwhen': 1,\n",
       "         'wil': 18,\n",
       "         'reach': 26,\n",
       "         'theory': 4,\n",
       "         'argument': 5,\n",
       "         'wins': 4,\n",
       "         'loses': 3,\n",
       "         'person': 40,\n",
       "         'argue': 4,\n",
       "         'kick': 5,\n",
       "         'correct': 8,\n",
       "         'tomarrow': 3,\n",
       "         'final': 2,\n",
       "         'hearing': 1,\n",
       "         'laptop': 12,\n",
       "         'case': 14,\n",
       "         'pleassssssseeeeee': 1,\n",
       "         'tel': 12,\n",
       "         'avent': 3,\n",
       "         'sportsx': 1,\n",
       "         'shining': 2,\n",
       "         'meant': 14,\n",
       "         'signing': 2,\n",
       "         'although': 2,\n",
       "         'told': 53,\n",
       "         'baig': 1,\n",
       "         'face': 29,\n",
       "         'watches': 1,\n",
       "         'watch': 33,\n",
       "         'fr': 13,\n",
       "         'thanx': 31,\n",
       "         'everything': 29,\n",
       "         'uve': 1,\n",
       "         'touched': 3,\n",
       "         'commercial': 2,\n",
       "         'website': 4,\n",
       "         'slippers': 3,\n",
       "         'asked': 22,\n",
       "         'kallis': 6,\n",
       "         'bat': 3,\n",
       "         'innings': 3,\n",
       "         'goodnight': 8,\n",
       "         'fix': 5,\n",
       "         'wake': 26,\n",
       "         'dearly': 3,\n",
       "         'missed': 20,\n",
       "         'sleep': 56,\n",
       "         'ranjith': 3,\n",
       "         'cal': 6,\n",
       "         'drpd': 2,\n",
       "         'deeraj': 2,\n",
       "         'deepak': 2,\n",
       "         '5min': 3,\n",
       "         'hold': 14,\n",
       "         'bcums': 4,\n",
       "         'angry': 18,\n",
       "         'wid': 16,\n",
       "         'dnt': 13,\n",
       "         'coz': 19,\n",
       "         'childish': 5,\n",
       "         'true': 22,\n",
       "         'showing': 7,\n",
       "         'deep': 10,\n",
       "         'affection': 4,\n",
       "         'care': 62,\n",
       "         'luv': 29,\n",
       "         'kettoda': 4,\n",
       "         'manda': 4,\n",
       "         'doinghow': 2,\n",
       "         'ups': 1,\n",
       "         '3days': 1,\n",
       "         'shipping': 3,\n",
       "         'takes': 12,\n",
       "         '2wks': 1,\n",
       "         'usps': 1,\n",
       "         'lag': 2,\n",
       "         'bribe': 1,\n",
       "         'nipost': 1,\n",
       "         'lemme': 8,\n",
       "         'necessarily': 2,\n",
       "         'expect': 4,\n",
       "         'headin': 2,\n",
       "         'mmm': 4,\n",
       "         'jolt': 2,\n",
       "         'suzy': 1,\n",
       "         'lover': 7,\n",
       "         'parked': 3,\n",
       "         'mini': 2,\n",
       "         'shopping': 23,\n",
       "         'disturb': 9,\n",
       "         'luton': 1,\n",
       "         '0125698789': 1,\n",
       "         'ring': 15,\n",
       "         'h': 1,\n",
       "         'dint': 6,\n",
       "         'wana': 11,\n",
       "         'plan': 32,\n",
       "         'trip': 17,\n",
       "         'sometme': 1,\n",
       "         'evo': 1,\n",
       "         'flash': 3,\n",
       "         'jealous': 3,\n",
       "         'sorting': 3,\n",
       "         'narcotics': 1,\n",
       "         'ended': 6,\n",
       "         'sunny': 6,\n",
       "         'rays': 5,\n",
       "         'leaves': 10,\n",
       "         'worries': 9,\n",
       "         'blue': 12,\n",
       "         'bay': 4,\n",
       "         'might': 34,\n",
       "         ...})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6152",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the code and understand what it does:\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "words = sms[sms.label == 'ham'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])\n",
    "ham_words = Counter()\n",
    "\n",
    "for msg in words:\n",
    "    ham_words.update(msg)\n",
    "\n",
    "print(ham_words.most_common(50))\n",
    "```\n",
    "\n",
    "This code aims to analyze the most common words in the text messages labeled as \"ham\" (meaning non-spam messages) in the DataFrame `sms`. Here's how it works:\n",
    "\n",
    "1. `from collections import Counter`: This imports the `Counter` class from the `collections` module. The `Counter` class is a convenient container for counting elements in a list or an iterable.\n",
    "\n",
    "2. `words = sms[sms.label == 'ham'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])`: This line filters the `sms` DataFrame to include only the rows where the `'label'` column is `'ham'` (non-spam messages). Then, it applies a lambda function to the `'clean_msg'` column of those filtered rows. The lambda function splits each message into a list of words and converts each word to lowercase. The result is a Series of lists, where each list contains the lowercase words of a message.\n",
    "\n",
    "3. `ham_words = Counter()`: This creates an instance of the `Counter` class called `ham_words`.\n",
    "\n",
    "4. `for msg in words:\n",
    "      ham_words.update(msg)`: This loop iterates over each list of words in the `words` Series. For each list, it updates the `ham_words` counter by counting the occurrences of each word.\n",
    "\n",
    "5. `print(ham_words.most_common(50))`: This prints the 50 most common words and their counts from the `ham_words` counter. The `most_common()` method of the `Counter` class returns a list of tuples, where each tuple contains a word and its count. The list is sorted in descending order based on the counts.\n",
    "\n",
    "The purpose of this code is to determine the most frequently used words in non-spam messages. By counting the occurrences of words in the `'clean_msg'` column for messages labeled as \"ham,\" the code helps identify common words that are likely to appear in legitimate (non-spam) messages. This information can be useful for various purposes, such as text classification, feature engineering, or understanding the characteristics of non-spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "724918e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('call', 347), ('free', 216), ('txt', 150), ('mobile', 123), ('text', 120), ('claim', 113), ('stop', 113), ('reply', 101), ('prize', 92), ('get', 83), ('new', 69), ('send', 67), ('nokia', 65), ('urgent', 63), ('cash', 62), ('win', 60), ('contact', 56), ('service', 55), ('please', 52), ('guaranteed', 50), ('customer', 49), ('16', 49), ('week', 49), ('tone', 48), ('per', 46), ('phone', 45), ('18', 43), ('chat', 42), ('awarded', 38), ('draw', 38), ('latest', 36), ('√•¬£1000', 35), ('line', 35), ('150ppm', 34), ('mins', 34), ('receive', 33), ('camera', 33), ('1', 33), ('every', 33), ('message', 32), ('holiday', 32), ('landline', 32), ('shows', 31), ('√•¬£2000', 31), ('go', 31), ('box', 30), ('number', 30), ('apply', 29), ('code', 29), ('live', 29)]\n"
     ]
    }
   ],
   "source": [
    "words = sms[sms.label=='spam'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])\n",
    "spam_words = Counter()\n",
    "\n",
    "for msg in words:\n",
    "    spam_words.update(msg)\n",
    "    \n",
    "print(spam_words.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5f0c6",
   "metadata": {},
   "source": [
    "### üßÆ Vectorization\n",
    "\n",
    "- Currently, we have the messages as lists of tokens (also known as lemmas) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.\n",
    "\n",
    "- Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n",
    "\n",
    "We'll do that in three steps using the bag-of-words model:\n",
    "\n",
    "Count how many times does a word occur in each message (Known as term frequency)\n",
    "Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "Let's begin the first step:\n",
    "\n",
    "- Each vector will have as many dimensions as there are unique words in the SMS corpus. We will first use SciKit Learn's CountVectorizer. This model will convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- We can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message.\n",
    "\n",
    "- For example:\n",
    "\n",
    "\n",
    "**Message 1\tMessage 2\t...\tMessage N\n",
    "     Word 1 Count\t0\t1\t...\t0\n",
    "     Word 2 Count\t0\t0\t...\t0\n",
    "     ...            1\t2\t...\t0\n",
    "     Word N Count\t0\t1\t...\t1**\n",
    " \n",
    "- Since there are so many messages, we can expect a lot of zero counts for the presence of that word in that document. Because of this, SciKit Learn will output a Sparse Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "017c891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,)\n",
      "(4179,)\n",
      "(1393,)\n",
      "(4179,)\n",
      "(1393,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "X=sms.clean_msg\n",
    "y=sms.label_num\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b113b",
   "metadata": {},
   "source": [
    "There are a lot of arguments and parameters that can be passed to the CountVectorizer. In this case we will just specify the analyzer to be our own previously defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5168912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'> (4179, 7996)\n",
      "<class 'scipy.sparse._csr.csr_matrix'> (1393, 7996)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect=CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "X_train_dtm=vect.transform(X_train)\n",
    "\n",
    "# equivalently: combine fit and transform into a single step\n",
    "X_train_dtm=vect.fit_transform(X_train)\n",
    "\n",
    "# examine the document-term matrix\n",
    "print(type(X_train_dtm), X_train_dtm.shape)\n",
    "\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm=vect.transform(X_test)\n",
    "print(type(X_test_dtm),X_test_dtm.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b269a64",
   "metadata": {},
   "source": [
    "<class 'scipy.sparse._csr.csr_matrix'>: This part of the output tells you the data type of the object you're dealing with. It indicates that you have a SciPy CSR matrix.\n",
    "\n",
    "(4179, 7996): These numbers inside the parentheses represent the shape of the CSR matrix. In this case, the first number (4179) represents the number of rows in the matrix, and the second number (7996) represents the number of columns in the matrix. So, the first matrix has 4179 rows and 7996 columns, while the second matrix has 1393 rows and 7996 columns.\n",
    "\n",
    "CSR matrices are a type of sparse matrix representation used to efficiently store matrices with a large number of zero elements. They store only the non-zero elements along with their row indices, making them memory-efficient for sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59d814f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7996 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 34796 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer.fit(X_train_dtm)\n",
    "tfidf_transformer.transform(X_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d37ef",
   "metadata": {},
   "source": [
    "### **TfidfTransformer**\n",
    "\n",
    "Certainly! Let's break down the code and explain the concept of `TfidfTransformer`:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer.fit(X_train_dtm)\n",
    "tfidf_transformer.transform(X_train_dtm)\n",
    "```\n",
    "\n",
    "The code snippet demonstrates the usage of `TfidfTransformer` from the scikit-learn library, which is a commonly used library for machine learning in Python. Here's what the code does:\n",
    "\n",
    "1. `from sklearn.feature_extraction.text import TfidfTransformer`: This imports the `TfidfTransformer` class from the `sklearn.feature_extraction.text` module. The `TfidfTransformer` class is used to compute the Term Frequency-Inverse Document Frequency (TF-IDF) representation of text data.\n",
    "\n",
    "2. `tfidf_transformer = TfidfTransformer()`: This creates an instance of the `TfidfTransformer` class and assigns it to the variable `tfidf_transformer`. The `TfidfTransformer` class is initialized with its default parameters.\n",
    "\n",
    "3. `tfidf_transformer.fit(X_train_dtm)`: The `fit()` method is called on the `tfidf_transformer` object, where `X_train_dtm` is the input data. The `fit()` method computes the IDF values required for the TF-IDF transformation. It learns the IDF weights from the training data.\n",
    "\n",
    "4. `tfidf_transformer.transform(X_train_dtm)`: The `transform()` method is called on the `tfidf_transformer` object with `X_train_dtm` as the input. This method applies the TF-IDF transformation to the input data. It computes the TF-IDF representation based on the learned IDF weights from the training data.\n",
    "\n",
    "The concept of TF-IDF:\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic used to evaluate the importance of a word in a document within a collection of documents. The TF-IDF value of a word increases proportionally to the number of times it appears in the document (term frequency) but is offset by the frequency of the word across all documents (inverse document frequency).\n",
    "\n",
    "The TF-IDF transformation is commonly used in text mining and natural language processing tasks. It gives more weight to terms that are frequent in a particular document but relatively rare in the entire document collection. This helps to identify terms that are potentially more informative and distinctive for a specific document.\n",
    "\n",
    "In the code snippet, the `TfidfTransformer` is fitted on the training data (`X_train_dtm`) to learn the IDF weights. Then, the same transformer is used to transform the training data, resulting in the TF-IDF representation of the text data.\n",
    "\n",
    "The TF-IDF representation can be useful for various downstream tasks, including text classification, information retrieval, and clustering, as it provides a numerical representation that captures the importance of words in the context of the document collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf743c6",
   "metadata": {},
   "source": [
    "### ü§ñ Building and evaluating a model\n",
    "We will use [multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549142a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bff3815",
   "metadata": {},
   "source": [
    "### why did we perform first text preprocessing then vectorizer then tf-idf transformer\n",
    "\n",
    "The order in which you perform text preprocessing, vectorization, and TF-IDF transformation in natural language processing (NLP) tasks is important and follows a logical sequence. Here's why you typically perform these steps in this order:\n",
    "\n",
    "**Text Preprocessing:**\n",
    "\n",
    "Cleaning and Tokenization: Initially, you clean the raw text data by removing irrelevant characters, punctuation, HTML tags, and other noise. Then, you tokenize the text, splitting it into individual words or tokens. Tokenization makes it easier to work with the text data at a granular level.\n",
    "\n",
    "**Lowercasing:** Converting all text to lowercase is a common preprocessing step to ensure that words are treated as the same regardless of their casing. This helps in reducing the vocabulary size and ensuring consistency.\n",
    "\n",
    "**Stop Word Removal:** Stop words are common words (e.g., \"the,\" \"and,\" \"is\") that often don't carry significant meaning and can be removed to reduce noise in the data.\n",
    "\n",
    "**Stemming or Lemmatization:** Reducing words to their base form (e.g., \"running\" to \"run\") through stemming or lemmatization helps in treating different forms of the same word as equivalent.\n",
    "\n",
    "**Other Specific Preprocessing:** Depending on the task, you might perform other preprocessing steps such as removing numbers, special characters, or domain-specific cleaning.\n",
    "\n",
    "The purpose of text preprocessing is to clean and prepare the text data for analysis and feature extraction. It simplifies the text while retaining important information.\n",
    "\n",
    "**Vectorization:**\n",
    "\n",
    "**Count Vectorization or TF-IDF Vectorization:** Once the text is preprocessed, you convert it into a numerical format that machine learning algorithms can understand. Common techniques include Count Vectorization (representing documents as word frequency counts) or TF-IDF Vectorization (representing documents as TF-IDF scores, which reflect the importance of words in documents relative to a corpus).\n",
    "\n",
    "**Word Embeddings:** Alternatively, you can use word embeddings like Word2Vec or GloVe to represent words as dense vectors in a continuous space. These embeddings capture semantic relationships between words.\n",
    "\n",
    "Vectorization turns text data into numerical features that can be used as input to machine learning models. It's a crucial step for most NLP tasks.\n",
    "\n",
    "**TF-IDF Transformation:**\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF): This transformation is often applied after vectorization. TF-IDF is used to weigh the importance of words in documents relative to a corpus of documents. It assigns higher values to words that are frequent in a document but not frequent across all documents, making it useful for identifying important terms in a document.\n",
    "\n",
    "**Normalization:** TF-IDF scores are usually normalized to ensure that the resulting vectors have a consistent scale, which can improve the performance of machine learning algorithms.\n",
    "\n",
    "TF-IDF is particularly useful for tasks like document retrieval, text classification, and information retrieval, where you want to identify important terms that discriminate between documents.\n",
    "\n",
    "In summary, the sequence of text preprocessing, vectorization, and TF-IDF transformation is designed to prepare and represent text data in a way that is suitable for machine learning tasks. Each step serves a specific purpose in the feature engineering process to make text data more amenable to modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebb12045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64363b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 12 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time nb.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3041e8",
   "metadata": {},
   "source": [
    "### MultinomialNB\n",
    "- The line %time nb.fit(X_train_dtm, y_train) is a magic command in IPython that times the execution of the following code, which is the training of a Multinomial Naive Bayes (MNB) model on the training data X_train_dtm and y_train.\n",
    "\n",
    "- Multinomial Naive Bayes is a probabilistic classifier that is based on the Bayes theorem and the assumption that the features in the data are independent of each other. MNB is a popular choice for text classification tasks because it is simple to implement and efficient to train and predict.\n",
    "\n",
    "- **X_train_dtm** is the document-term matrix of the training data, which is a representation of the text data where each row represents a document and each column represents a word. The elements of the matrix represent the number of times each word appears in each document.\n",
    "\n",
    "- **y_train** is the target vector of the training data, which contains the labels for each document.\n",
    "\n",
    "- **The fit() method of the MNB classifier takes the training data X_train_dtm and y_train as input and learns the parameters of the model. The trained model can then be used to predict the labels for new data.**\n",
    "\n",
    "- The %time magic command in IPython outputs the execution time of the following code in milliseconds. This can be useful for measuring the performance of different machine learning algorithms or for optimizing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d54881b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Accuracy Score===========\n",
      "0.9827709978463748\n",
      "=======Confision Matrix===========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1205,    8],\n",
       "       [  16,  164]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class=nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy of class predictions\n",
    "print(\"=======Accuracy Score===========\")\n",
    "print(metrics.accuracy_score(y_test,y_pred_class))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"=======Confision Matrix===========\")\n",
    "metrics.confusion_matrix(y_test,y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b87f3",
   "metadata": {},
   "source": [
    "The output you provided is for the Multinomial Naive Bayes algorithm, and it includes the accuracy score and confusion matrix. Let's break down the output and explain each part:\n",
    "\n",
    "```\n",
    "=======Accuracy Score===========\n",
    "0.9827709978463748\n",
    "```\n",
    "\n",
    "The accuracy score is a metric used to evaluate the performance of a classification model. It represents the proportion of correctly predicted labels (both true positives and true negatives) out of the total number of instances.\n",
    "\n",
    "In your case, the accuracy score is 0.9827709978463748, which indicates that the Multinomial Naive Bayes algorithm achieved an accuracy of approximately 98.28%. This means that the model accurately predicted the labels for the majority of the instances in the dataset.\n",
    "\n",
    "```\n",
    "=======Confusion Matrix===========\n",
    "array([[1205,    8],\n",
    "       [  16,  164]], dtype=int64)\n",
    "```\n",
    "\n",
    "The confusion matrix is a table that summarizes the performance of a classification model by showing the counts of true positives, true negatives, false positives, and false negatives. It is a useful tool for evaluating the performance of a model across different classes.\n",
    "\n",
    "In your case, the confusion matrix is presented as a 2x2 array:\n",
    "\n",
    "- The top-left value, 1205, represents the count of true negatives (TN). These are instances that are actually negative (in the negative class) and were correctly classified as negative by the model.\n",
    "- The top-right value, 8, represents the count of false positives (FP). These are instances that are actually negative but were incorrectly classified as positive by the model.\n",
    "- The bottom-left value, 16, represents the count of false negatives (FN). These are instances that are actually positive but were incorrectly classified as negative by the model.\n",
    "- The bottom-right value, 164, represents the count of true positives (TP). These are instances that are actually positive (in the positive class) and were correctly classified as positive by the model.\n",
    "\n",
    "The confusion matrix provides a more detailed understanding of the model's performance by showing the distribution of correct and incorrect predictions across the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c40f78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2418    Madamregret disturbancemight receive reference...\n",
       "4598                                laid airtel line rest\n",
       "386                                   Customer place call\n",
       "1289    HeyGreat dealFarm tour 9am 5pm 95pax 50 deposi...\n",
       "5094    Hi ShanilRakhesh herethanksi exchanged uncut d...\n",
       "494                                      free nowcan call\n",
       "759     Call youcarlos isare phones vibrate acting mig...\n",
       "3140                                  Customer place call\n",
       "Name: clean_msg, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for false positives (ham incorrectly classifier)\n",
    "# X_test[(y_pred_class==1) & (y_test==0)]\n",
    "X_test[y_pred_class>y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c474bf35",
   "metadata": {},
   "source": [
    "### Above code Explanation\n",
    "The code snippet you provided aims to identify the messages that were incorrectly classified as \"ham\" (non-spam) but are actually \"spam\" (false positives). Let's break down the code and understand its logic:\n",
    "\n",
    "```python\n",
    "X_test[y_pred_class > y_test]\n",
    "```\n",
    "\n",
    "In this code, `X_test` represents the test data, which likely contains the message texts. `y_pred_class` represents the predicted labels for the test data, and `y_test` represents the true labels of the test data.\n",
    "\n",
    "The logic behind this code is to compare the predicted labels (`y_pred_class`) with the true labels (`y_test`) using a comparison operator (`>`). By doing so, it creates a Boolean mask that is `True` where the predicted label is greater than the true label. \n",
    "\n",
    "Since the labels are binary (0 for \"ham\" and 1 for \"spam\"), if `y_pred_class` is greater than `y_test`, it means that the message was classified as \"ham\" (0) when it should have been classified as \"spam\" (1). These are the instances that correspond to false positives.\n",
    "\n",
    "Finally, by indexing `X_test` with this Boolean mask, you will retrieve the message texts for the false positive instances. This will provide you with the content of the messages that were incorrectly classified as \"ham\" but should have been classified as \"spam\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b2f04",
   "metadata": {},
   "source": [
    "### Reason  to identify the false positves\n",
    "Identifying false positives is important for evaluating the performance of a classification model, particularly in the context of spam detection or any application where it is crucial to minimize the occurrence of false positives.\n",
    "\n",
    "False positives occur when the model incorrectly predicts a positive (spam) class when the true class is negative (ham). In the case of spam detection, a false positive means that a legitimate message is flagged as spam, potentially leading to important messages being missed or filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1202e03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4674    Hi babe Chloe r smashed saturday night great w...\n",
       "3528    Xmas New Years Eve tickets sale club day 10am ...\n",
       "3417    LIFE never much fun great came made truly spec...\n",
       "2773    come takes little time child afraid dark becom...\n",
       "1960    Guess Somebody know secretly fancies Wanna fin...\n",
       "5       FreeMsg Hey darling 3 weeks word back Id like ...\n",
       "2078                         85233 FREERingtoneReply REAL\n",
       "1457    CLAIRE havin borin time alone wanna cum 2nite ...\n",
       "190     unique enough Find 30th August wwwareyouunique...\n",
       "2429    Guess IThis first time created web page WWWASJ...\n",
       "3057    unsubscribed services Get tons sexy babes hunk...\n",
       "1021    Guess Somebody know secretly fancies Wanna fin...\n",
       "4067    TBSPERSOLVO chasing us since Sept for√•¬£38 defi...\n",
       "3358         Sorry missed call lets talk time 07090201529\n",
       "2821    ROMCAPspam Everyone around responding well pre...\n",
       "2247    Back work 2morro half term C 2nite sexy passio...\n",
       "Name: clean_msg, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for false negatives (spam incorrectly classifier)\n",
    "X_test[y_pred_class < y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f54616b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi probably much fun get message thought id txt cos bored james farting night'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of false negative \n",
    "X_test[4949]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "396b2a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.11903975e-02, 3.97831612e-04, 1.06470895e-03, ...,\n",
       "       1.31939653e-02, 9.99821127e-05, 6.04083365e-06])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (poorly calibrated) = The predict_proba() method returns an array containing a row per instance and a column per class, each containing the probability that the given instance belongs to the given class\n",
    "y_pred_prob=nb.predict_proba(X_test_dtm)[:,1] #But to plot a ROC curve, you need scores, not probabilities. A simple solution is to use the positive class‚Äôs probability as the score here iam calculating predict probability bu using the X_test data which is predicted data and in the X_test there are 2 labels which are 0 ham and spam 1 iam taking only the spam 1 label and taking all the rows of the spam column in order yo plot a roc curve which it gives us a plot b/w true_postive and false positive   \n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354ec20",
   "metadata": {},
   "source": [
    "### Prediction Probability\n",
    "In the code snippet you provided, `y_pred_prob = nb.predict_proba(X_test_dtm)[:,1]` calculates the predicted probabilities of the positive class (spam) for the test data (`X_test_dtm`) using the trained Naive Bayes classifier (`nb`). Let's break down the code and explain its purpose:\n",
    "\n",
    "```python\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:,1]\n",
    "```\n",
    "\n",
    "1. `nb.predict_proba(X_test_dtm)`: This line uses the trained Naive Bayes classifier (`nb`) to predict the class probabilities for each instance in the test data (`X_test_dtm`). The `predict_proba()` method returns a matrix of shape `(n_samples, n_classes)`, where `n_samples` is the number of instances in the test data and `n_classes` is the number of classes (in this case, 2: ham and spam). Each entry in the matrix represents the probability of the corresponding instance belonging to a particular class.\n",
    "\n",
    "2. `[:,1]`: This indexing operation selects the second column (`[:,1]`) from the predicted probability matrix. In scikit-learn, the columns correspond to the classes, so `[:,1]` retrieves the predicted probabilities of the positive class (spam). The indexing creates a 1-dimensional array (`y_pred_prob`) containing only the predicted probabilities of the positive class.\n",
    "\n",
    "The purpose of calculating predicted probabilities is to obtain a continuous measure of certainty/confidence in the model's predictions. By getting the predicted probability of the positive class (spam), you can assess the likelihood of an instance being classified as spam.\n",
    "\n",
    "The predicted probabilities can be useful in various scenarios, such as:\n",
    "\n",
    "- Setting a custom classification threshold: Instead of using the default threshold of 0.5 for binary classification, you can choose a different threshold based on the predicted probabilities. For example, if you want to prioritize precision (reduce false positives), you can set a higher threshold to classify instances as spam only when the predicted probability is above a certain value.\n",
    "\n",
    "- Evaluating model performance: Predicted probabilities can be used to calculate different evaluation metrics like ROC-AUC, precision-recall curve, or log loss, which provide a more nuanced assessment of the model's performance compared to just accuracy.\n",
    "\n",
    "- Ranking or prioritizing instances: Predicted probabilities can be used to rank instances by their likelihood of being in a certain class. For example, in spam detection, you can prioritize reviewing or handling messages with higher predicted probabilities of being spam.\n",
    "\n",
    "By computing and analyzing predicted probabilities, you can gain additional insights into the model's predictions and make more informed decisions based on the confidence levels associated with each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55b55226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9774342768159751"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate AUC`\n",
    "metrics.roc_auc_score(y_test,y_pred_prob) # roc curve show the plot b/w TP and FP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28663126",
   "metadata": {},
   "source": [
    "### concept of AUC (Area Under the ROC Curve)\n",
    "The concept of AUC (Area Under the ROC Curve) is a commonly used evaluation metric for binary classification problems, which measures the performance of a model in distinguishing between positive and negative instances. Let's break down the concept of AUC and explain why we calculate `roc_auc_score` using the predicted probabilities.\n",
    "\n",
    "AUC measures the overall performance of a model across various classification thresholds. Here's how it works:\n",
    "\n",
    "1. ROC Curve: ROC (Receiver Operating Characteristic) curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various classification thresholds. TPR is also known as sensitivity or recall, and it represents the proportion of true positive instances correctly classified as positive. FPR is the proportion of true negative instances incorrectly classified as positive.\n",
    "\n",
    "2. AUC Calculation: AUC is the area under the ROC curve, ranging from 0 to 1. It quantifies the trade-off between TPR and FPR across all possible classification thresholds. A model with a higher AUC value indicates better discrimination power and overall performance in distinguishing between positive and negative instances.\n",
    "\n",
    "Now, let's address why we calculate `roc_auc_score` using the predicted probabilities (`y_pred_prob`):\n",
    "\n",
    "```python\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "```\n",
    "\n",
    "1. `y_test`: `y_test` represents the true labels of the test data. It contains the actual class labels for the instances in the test set.\n",
    "\n",
    "2. `y_pred_prob`: `y_pred_prob` is the predicted probability of the positive class (spam) that we obtained using the model. It contains the predicted probabilities for each instance in the test set.\n",
    "\n",
    "By calculating `roc_auc_score` using `y_test` and `y_pred_prob`, we evaluate the performance of the model based on the predicted probabilities. It quantifies how well the model ranks the instances in terms of their likelihood of being positive (spam). Here's how `roc_auc_score` works:\n",
    "\n",
    "- `roc_auc_score` takes the true labels (`y_test`) and the predicted probabilities (`y_pred_prob`) as input.\n",
    "\n",
    "- It computes the ROC curve using the true labels and predicted probabilities.\n",
    "\n",
    "- Then, it calculates the AUC by computing the area under the ROC curve.\n",
    "\n",
    "- The resulting AUC score indicates the overall performance of the model in terms of its ability to distinguish between positive and negative instances. A value closer to 1 indicates better performance, while a value closer to 0.5 suggests a weaker model.\n",
    "\n",
    "By calculating `roc_auc_score`, we gain insights into the model's ability to rank instances correctly and discriminate between positive and negative classes. It provides a single numeric value that summarizes the model's performance and allows for easy comparison with other models or evaluation of different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7fee056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Accuracy Score===========\n",
      "0.9669777458722182\n",
      "=======Confision Matrix===========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1213,    0],\n",
       "       [  46,  134]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('bow', CountVectorizer()), \n",
    "                 ('tfid', TfidfTransformer()),  \n",
    "                 ('model', MultinomialNB())])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# calculate accuracy of class predictions\n",
    "print(\"=======Accuracy Score===========\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"=======Confision Matrix===========\")\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00390d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e77a47",
   "metadata": {},
   "source": [
    "### üìä Comparing models\n",
    "We will compare multinomial Naive Bayes with logistic regression:\n",
    "\n",
    "Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed2acf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 78.5 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import an instantiate a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression(solver='liblinear')\n",
    "\n",
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c83e4bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01694418, 0.0152182 , 0.08261755, ..., 0.02198942, 0.00531726,\n",
       "       0.00679188])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class=logreg.predict(X_test_dtm)\n",
    "\n",
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dbea797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Accuracy Score===========\n",
      "0.9842067480258435\n",
      "=======Confision Matrix===========\n",
      "[[1213    0]\n",
      " [  22  158]]\n",
      "=======ROC AUC Score===========\n",
      "0.9835714940001832\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "print(\"=======Accuracy Score===========\")\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"=======Confision Matrix===========\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "# calculate AUC\n",
    "print(\"=======ROC AUC Score===========\")\n",
    "print(metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de49f964",
   "metadata": {},
   "source": [
    "The confusion matrix and accuracy score values can change before and after performing the ROC curve analysis because the ROC curve analysis involves using the predicted probabilities to calculate scores, which can result in different classification thresholds and corresponding predictions.\n",
    "\n",
    "In the first output:\n",
    "\n",
    "```\n",
    "=======Accuracy Score===========\n",
    "0.9827709978463748\n",
    "=======Confusion Matrix===========\n",
    "array([[1205,    8],\n",
    "       [  16,  164]], dtype=int64)\n",
    "```\n",
    "\n",
    "The accuracy score is approximately 98.28%, indicating a high accuracy of the model. The confusion matrix shows 1205 true negatives, 164 true positives, 8 false positives, and 16 false negatives. The model correctly classified a majority of instances, with few errors.\n",
    "\n",
    "In the second output:\n",
    "\n",
    "```\n",
    "=======Accuracy Score===========\n",
    "0.9669777458722182\n",
    "=======Confusion Matrix===========\n",
    "array([[1213,    0],\n",
    "       [  46,  134]], dtype=int64)\n",
    "```\n",
    "\n",
    "The accuracy score is approximately 96.70%, which is slightly lower than the previous accuracy score. The confusion matrix shows 1213 true negatives, 134 true positives, 0 false positives, and 46 false negatives. The model correctly classified a majority of instances, but there is an increase in false negatives and a reduction in false positives compared to the previous output.\n",
    "\n",
    "The change in values can be attributed to the use of predicted probabilities and the classification thresholds employed when calculating the ROC curve. Different thresholds can result in different classifications, leading to variations in the confusion matrix and accuracy scores.\n",
    "\n",
    "When performing email spam filtering in a data science project, some important considerations include:\n",
    "\n",
    "1. Data preprocessing: Preprocess the email data by cleaning the text, removing unnecessary information (e.g., HTML tags), and transforming the text into a suitable format for analysis (e.g., tokenization, stemming, or lemmatization).\n",
    "\n",
    "2. Feature extraction: Extract relevant features from the email data to represent its content effectively. Common approaches include bag-of-words representation, TF-IDF (Term Frequency-Inverse Document Frequency), or more advanced techniques like word embeddings.\n",
    "\n",
    "3. Model selection: Choose an appropriate machine learning algorithm for classification, such as Naive Bayes, Support Vector Machines (SVM), or Random Forests, based on the specific requirements and characteristics of the data.\n",
    "\n",
    "4. Model evaluation: Assess the performance of the model using appropriate evaluation metrics like accuracy, precision, recall, F1-score, and AUC-ROC. Consider the trade-off between false positives and false negatives and choose a threshold that balances the two based on the project's requirements.\n",
    "\n",
    "5. Validation and testing: Split the dataset into training and testing sets to evaluate the model's generalization ability. Perform cross-validation or use a separate validation set to fine-tune hyperparameters and avoid overfitting.\n",
    "\n",
    "6. Iterative improvement: Analyze misclassified instances, review false positives and false negatives, and iteratively refine the model by adjusting the feature representation, applying ensemble methods, or incorporating additional data sources.\n",
    "\n",
    "By considering these factors and continuously improving the model, you can develop an effective email spam filter that accurately classifies emails and minimizes the occurrence of false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f74f717",
   "metadata": {},
   "source": [
    "### üßÆ Tuning the vectorizer\n",
    "Thus far, we have been using the default parameters of [CountVectorizer:](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ea859b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show default parameters for CountVectorizer\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9635ff2",
   "metadata": {},
   "source": [
    "üìå However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:\n",
    "\n",
    "üìå **stop_words:** string {'english'}, list, or None (default)\n",
    "If 'english', a built-in stop word list for English is used.\n",
    "If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45735373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect=CountVectorizer(stop_words='english') #  you are instructing the CountVectorizer to remove common English stop words (e.g., \"the,\" \"is,\" \"and\") from the text data. Stop words are frequently occurring words that often do not carry much useful information for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38ef23",
   "metadata": {},
   "source": [
    "- üìå **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "- All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect=CountVectorizer(ngram_range=(1,2)) # means that both single words (1-grams) and pairs of consecutive words (2-grams) will be considered as features. This allows the model to capture not only individual words but also combinations of words that might carry important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed724d0f",
   "metadata": {},
   "source": [
    "- üìå **max_df:** float in range [0.0, 1.0] or int, default=1.0\n",
    "- When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "- If float, the parameter represents a proportion of documents.\n",
    "- If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18414a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect=CountVectorizer(max_df=0.5) # means that any term that appears in more than 50% of the documents will be ignored as it is considered too common to provide meaningful information for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667dc5f",
   "metadata": {},
   "source": [
    "- üìå **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n",
    "- If float, the parameter represents a proportion of documents.\n",
    "- If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fd1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep terms that appear in at least 2 documents\n",
    "vect=CountVectorizer(min_df=2) # This helps filter out rare or unique terms that may not provide enough information for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50984060",
   "metadata": {},
   "source": [
    "- üìå **Guidelines for tuning CountVectorizer:**\n",
    "- Use your knowledge of the problem and the text, and your understanding of the tuning parameters, to help you decide what parameters to tune and how to tune them.\n",
    "- Experiment, and let the data tell you the best approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b00ea6",
   "metadata": {},
   "source": [
    "In the given code snippets, you are tuning the `CountVectorizer` object, which is a common technique used in natural language processing (NLP) tasks like email spam filtering. The `CountVectorizer` is responsible for converting text documents into a matrix representation of token counts.\n",
    "\n",
    "Let's go through each code snippet and explain what each parameter does:\n",
    "\n",
    "1. `vect = CountVectorizer(stop_words='english')`\n",
    "   By setting the `stop_words` parameter to `'english'`, you are instructing the `CountVectorizer` to remove common English stop words (e.g., \"the,\" \"is,\" \"and\") from the text data. Stop words are frequently occurring words that often do not carry much useful information for classification tasks.\n",
    "\n",
    "2. `vect = CountVectorizer(ngram_range=(1,2))`\n",
    "   The `ngram_range` parameter specifies the range of n-grams to be considered. An n-gram is a contiguous sequence of n items (words, characters, etc.) from a given sample of text. In this case, setting `ngram_range=(1,2)` means that both single words (1-grams) and pairs of consecutive words (2-grams) will be considered as features. This allows the model to capture not only individual words but also combinations of words that might carry important contextual information.\n",
    "\n",
    "3. `vect = CountVectorizer(max_df=0.5)`\n",
    "   The `max_df` parameter specifies the maximum document frequency of a term. It represents the threshold for ignoring terms that appear in more than a certain percentage of the documents. In this case, `max_df=0.5` means that any term that appears in more than 50% of the documents will be ignored as it is considered too common to provide meaningful information for classification.\n",
    "\n",
    "4. `vect = CountVectorizer(min_df=2)`\n",
    "   The `min_df` parameter specifies the minimum document frequency of a term. It represents the threshold for ignoring terms that appear in fewer documents than the specified value. In this case, `min_df=2` means that only terms that appear in at least two documents will be considered as features. This helps filter out rare or unique terms that may not provide enough information for classification.\n",
    "\n",
    "By tuning these parameters of the `CountVectorizer`, you can customize the text representation and potentially improve the performance of your email spam filtering model. However, it's important to note that the optimal parameter values may vary depending on the specific characteristics of your dataset, so it's recommended to experiment and compare different configurations to find the best settings for your particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304a3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
